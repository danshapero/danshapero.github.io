{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, we'll look at the *obstacle problem*.\n",
    "We've seen in previous posts examples of variational problems -- minimization of some functional with respect to a field.\n",
    "The classic example of a variational problem is to find the function $u$ that minimizes the Dirichlet energy\n",
    "\n",
    "$$\\mathscr{J}(u) = \\int_\\Omega\\left(\\frac{1}{2}|\\nabla u|^2 - fu\\right)dx$$\n",
    "\n",
    "subject to the homogeneous Dirichlet boundary condition $u|_{\\partial\\Omega} = 0$.\n",
    "The Poisson equation is especially convenient because the objective is convex and quadratic.\n",
    "The obstacle problem is what you get when you add the additional constraint\n",
    "\n",
    "$$u \\ge g$$\n",
    "\n",
    "throughout the domain.\n",
    "More generally, we can look at the problem of minimizing a convex functional $\\mathscr{J}$ subject to the constraint that $u$ has to live in a closed, convex set $K$ of a function space $Q$.\n",
    "For a totally unconstrained problem, $K$ would just be the whole space $Q$.\n",
    "\n",
    "Newton's method with line search is a very effective algorithm for solving unconstrained convex problems, even for infinite-dimensional problems like PDEs.\n",
    "Things get much harder when you include inequality constraints.\n",
    "To make matters worse, much of the literature you'll find on this subject is focused on finite-dimensional problems, where techniques like the active-set method work quite well.\n",
    "It's not so obvious how to generalize these methods to variational problems.\n",
    "In the following, I'll follow the approach in section 4.1 of [this paper](https://www.tandfonline.com/doi/full/10.1080/10556788.2019.1613655) by Farrell, Croci, and Surowiec, whch was my inspiration for writing this post.\n",
    "\n",
    "Minimizing the action functional $\\mathscr{J}$ over the convex set $K$ can be rephrased as an unconstrained problem to minimize the functional\n",
    "\n",
    "$$\\mathscr{J}(u) + \\mathscr{I}(u),$$\n",
    "\n",
    "where $\\mathscr{I}$ is the *indicator function* of the set $K$:\n",
    "\n",
    "$$\\mathscr{I}(u) = \\begin{cases}0 & u \\in K \\\\ \\infty & u \\notin K\\end{cases}.$$\n",
    "\n",
    "This functional is still convex, but it can take the value $\\infty$.\n",
    "The reformulation isn't especially useful by itself, but we can approximate it using the *Moreau envelope*.\n",
    "The envelope of $\\mathscr{I}$ is defined as\n",
    "\n",
    "$$\\mathscr{I}_\\gamma(u) = \\min_v\\left(\\mathscr{I}(v) + \\frac{1}{2\\gamma^2}\\|u - v\\|^2\\right).$$\n",
    "\n",
    "In the limit as $\\gamma \\to 0$, $\\mathscr{I}_\\gamma(u) \\to \\mathscr{I}(u)$.\n",
    "The Moreau envelope is much easier to work with than the original functional because it's differentiable.\n",
    "In some cases it can be computed analytically; for example, when $\\mathscr{I}$ is an indicator function,\n",
    "\n",
    "$$\\mathscr{I}_\\gamma(u) = \\frac{1}{2\\gamma^2}\\text{dist}\\,(u, K)^2$$\n",
    "\n",
    "where $\\text{dist}$ is the distance to a convex set.\n",
    "We can do even better for our specific case, where $K$ is the set of all functions greater than $g$.\n",
    "For this choice of $K$, the distance to $K$ is\n",
    "\n",
    "$$\\text{dist}(u, K)^2 = \\int_\\Omega(u - g)_-^2dx,$$\n",
    "\n",
    "where $v_- = \\min(v, 0)$ is the negative part of $v$.\n",
    "So, our approach to solving the obstacle problem will be to find the minimzers of\n",
    "\n",
    "$$\\mathscr{J}_\\gamma(u) = \\int_\\Omega\\left(\\frac{1}{2}|\\nabla u|^2 - fu\\right)dx + \\frac{1}{2\\gamma^2}\\int_\\Omega(u - g)_-^2dx$$\n",
    "\n",
    "as $\\gamma$ goes to 0.\n",
    "I've written things in such a way that $\\gamma$ has units of length.\n",
    "Rather than take $\\gamma$ to 0 we can instead stop at some fraction of the finite element mesh spacing.\n",
    "At that point, the errors in the finite element approximation are comparable to the distance of the approximate solution to the constraint set.\n",
    "\n",
    "This is a lot like the penalty method for optimization problems with equality constraints.\n",
    "One of the main practical considerations when applying this regularization method is that the solution $u$ only satisfies the inequality constraints approximately.\n",
    "For the obstacle problem this deficiency isn't so severe, but for other problems we may need the solution to stay strictly feasible.\n",
    "In those cases, another approach like the logarithmic barrier method might be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration\n",
    "\n",
    "For our problem, the domain will be the unit square and the obstacle function $g$ will be the upper half of a sphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import firedrake\n",
    "nx, ny = 64, 64\n",
    "mesh = firedrake.UnitSquareMesh(nx, ny, quadrilateral=True)\n",
    "Q = firedrake.FunctionSpace(mesh, family='CG', degree=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firedrake import max_value, sqrt, inner, as_vector, Constant\n",
    "\n",
    "def make_obstacle(mesh):\n",
    "    x = firedrake.SpatialCoordinate(mesh)\n",
    "    y = as_vector((1/2, 1/2))\n",
    "    z = 1/4\n",
    "    return sqrt(max_value(z**2 - inner(x - y, x - y), 0))\n",
    "\n",
    "g = firedrake.Function(Q).interpolate(make_obstacle(mesh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d\n",
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(projection='3d')\n",
    "firedrake.trisurf(g, axes=axes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll make a few utility procedures to create the Moreau envelope of the objective functional and to calculate a search direction from a given starting guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firedrake import grad, dx, min_value\n",
    "\n",
    "def make_objective(u, g, γ):\n",
    "    J_elastic = 0.5 * inner(grad(u), grad(u)) * dx\n",
    "    J_penalty = 0.5 / γ**2 * min_value(u - g, 0)**2 * dx\n",
    "    return J_elastic + J_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firedrake import derivative\n",
    "def update_search_direction(J, u, v):\n",
    "    F = derivative(J, u)\n",
    "    H = derivative(F, u)\n",
    "\n",
    "    bc = firedrake.DirichletBC(u.function_space(), 0, 'on_boundary')\n",
    "    params = {'ksp_type': 'cg', 'pc_type': 'icc'}\n",
    "    firedrake.solve(H == -F, v, bc, solver_parameters=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start from a zero initial guess and see what the first search direction will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = firedrake.Function(Q)\n",
    "γ = Constant(1.)\n",
    "J = make_objective(u, g, γ)\n",
    "\n",
    "v = firedrake.Function(Q)\n",
    "update_search_direction(J, u, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(projection='3d')\n",
    "firedrake.trisurf(v, axes=axes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure that a Newton-type method will converge, we'll need a routine to perform a 1D minimization along the search direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "from firedrake import assemble, replace\n",
    "\n",
    "def line_search(J, u, v):\n",
    "    def J_line(step):\n",
    "        t = firedrake.Constant(step)\n",
    "        J_t = replace(J, {u: u + t * v})\n",
    "        return assemble(J_t)\n",
    "\n",
    "    result = minimize_scalar(J_line)\n",
    "    assert result.success\n",
    "    return result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = line_search(J, u, v)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these steps out of the way we can define a Newton search procedure and calculate a solution for our initial, rough guess of $\\gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firedrake import action\n",
    "def newton_search(J, u, tolerance=1e-10, max_num_steps=30):\n",
    "    v = firedrake.Function(u.function_space())\n",
    "    F = derivative(J, u)\n",
    "\n",
    "    for step in range(max_num_steps):\n",
    "        update_search_direction(J, u, v)\n",
    "\n",
    "        Δ = assemble(action(F, v))\n",
    "        if abs(Δ) < tolerance * assemble(J):\n",
    "            return\n",
    "\n",
    "        t = Constant(line_search(J, u, v))\n",
    "        u.assign(u + t * v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newton_search(J, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "axes = fig.add_subplot(projection='3d')\n",
    "firedrake.trisurf(u, axes=axes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution we obtain doesn't do a good job of staying above the obstacle because we haven't used a sufficiently small value of $\\gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "δ = firedrake.Function(Q).interpolate(max_value(g - u, 0))\n",
    "print(firedrake.assemble(δ * dx) / firedrake.assemble(g * dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we can use the solution obtained for one value of $\\gamma$ to initialize a search for the solution with $\\gamma / 2$ and iterate.\n",
    "We've chosen this slightly indirect route rather than start from a small value of $\\gamma$ directly because the problem may be very poorly conditioned.\n",
    "The numerical continuation approach can still give a reasonable answer even for poorly-conditioned problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuation_search(g, γ0, num_steps, contraction=0.5):\n",
    "    u = g.copy(deepcopy=True)\n",
    "    γ = Constant(γ0)\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        J = make_objective(u, g, γ)\n",
    "        newton_search(J, u)\n",
    "        γ.assign(contraction * γ)\n",
    "\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll choose a number of steps so that the final value of $\\gamma$ is roughly proportional to the mesh spacing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "num_steps = int(np.log2(nx)) + 1\n",
    "print(num_steps)\n",
    "\n",
    "u = continuation_search(g, 1., num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I'll plot a cross section of the solution and the constraint $g$ so that you can see where the two coincide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots()\n",
    "num_points = 51\n",
    "xs = np.linspace(0., 1., num_points)\n",
    "ys = 0.5 * np.ones(num_points)\n",
    "X = np.array((xs, ys)).T\n",
    "axes.plot(xs, g.at(X), color='tab:orange')\n",
    "axes.plot(xs, u.at(X), color='tab:blue');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement\n",
    "\n",
    "The code above worked well enough for a single grid, but one of the hard parts about optimization with PDE constraints is making sure that our algorithms do sane things under mesh refinement.\n",
    "Many common algorithms can have different convergence rates depending on the mesh or the degree of the finite element basis.\n",
    "The reasons for this are a little involved, but if you want to read more, I recommend [this book](https://epubs.siam.org/doi/book/10.1137/1.9781611973846) by Málek and Strakos.\n",
    "\n",
    "To really make sure we're doing things right, we should run this experiment at several levels of mesh refinement.\n",
    "We can do this easily using the `MeshHierarchy` function in Firedrake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_mesh = firedrake.UnitSquareMesh(nx, ny, quadrilateral=True)\n",
    "num_levels = 3\n",
    "mesh_hierarchy = firedrake.MeshHierarchy(coarse_mesh, num_levels)\n",
    "\n",
    "for level, mesh in enumerate(mesh_hierarchy):\n",
    "    Q = firedrake.FunctionSpace(mesh, family='CG', degree=1)\n",
    "    g = firedrake.Function(Q).interpolate(make_obstacle(mesh))\n",
    "    num_continuation_steps = int(np.log(nx)) + level + 1\n",
    "    u = continuation_search(g, 1, num_continuation_steps)\n",
    "    print(assemble(max_value(g - u, 0) * dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plot the volume of the region where $u$ is less than $g$, it decreases roughly by a factor of four on every mesh refinement.\n",
    "This rate of decrease makes sense -- the area of each cell decreases by the same amount on each refinement.\n",
    "Doing a more thorough convergence study would require more computational power, but for now this is a promising sign that our algorithm works right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "We were able to get a convergent approximation scheme for the obstacle problem by expressing the constraint as an indicator functional and then using Moreau-Yosida regularization.\n",
    "The idea of regularizing non-smooth optimization problems is a more general trick; we can use it for things like $L^1$ or total variation penalties as well.\n",
    "The Moreau envelope is another angle to look at [proximal algorithms](https://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf) from too.\n",
    "\n",
    "For the obstacle problem, regularization made it possible to describe every part of the algorithm using higher-level concepts (fields, functionals) without having to dive down to lower levels of abstraction (matrices, vectors).\n",
    "In order to implement other approaches, like the active set method, we would have no choice but to pull out the PETSc matrices and vectors that lie beneath, which is a more demanding prospect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firedrake",
   "language": "python",
   "name": "firedrake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "nikola": {
   "category": "",
   "date": "2019-11-22 13:26:50 UTC-08:00",
   "description": "",
   "link": "",
   "slug": "obstacle-problem",
   "tags": "",
   "title": "The obstacle problem",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
