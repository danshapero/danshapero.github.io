{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous post, I described a procedure for sampling from complicated probability distributions.\n",
    "There's a beautiful and decades-old theory for how to sample from probability distributions defined over finite-dimensional vector spaces using Monte Carlo methods.\n",
    "One of the subtle points that I tried to address there is what happens when the quantity we want to sample is a function of space and time.\n",
    "For example, we might want to infer the conductivity of an aquifer from well head measurements; the material density in the earth's subsurface from satellite gravimetry; or basal drag between a glacier and the landscape from measurements of its surface velocity.\n",
    "The parameter space is now a space of functions, and so it has infinitely many dimensions.\n",
    "Extending common algorithms to work in function spaces is hard.\n",
    "\n",
    "Since writing that first post, I read [this paper](https://doi.org/10.1214/13-STS421) by Cotter and others.\n",
    "I think my previous post was wrong in a few respects now.\n",
    "Here I'd like to explain why (it involves Weyl's law) and how to fix it (it involves Nitsche's method).\n",
    "\n",
    "The failure is trying to bolt a statistical interpretation onto deterministic inverse problems.\n",
    "I'm guilty of this.\n",
    "I'd argue that many textbooks on inverse problems are too.\n",
    "One misstep is to assume that common choices of regularization functional used in the deterministic inverse problems literature are going to translate into sensible prior distributions.\n",
    "They don't.\n",
    "\n",
    "To clarify the notation for later, the quantity we're interested in is a field $q$ defined over some spatial domain $\\Omega$.\n",
    "This field lives in some separable Hilbert space $H$, which is most likely the space of square-integrable functions $L^2(\\Omega)$.\n",
    "But it could also be the Sobolev space $W_1^2(\\Omega)$ or something else.\n",
    "The field is assumed to take random values determined by some probability distribution $\\pi(q)$.\n",
    "In a deterministic inverse problem, we would add a multiple of a functional $R(q)$ to the objective in order to regularize the problem.\n",
    "The conventional approach is to use a multiple of\n",
    "$$R(q) = \\frac{1}{2}\\int_\\Omega|\\nabla q|^2dx.$$\n",
    "We can make this look like a statistical inference problem by taking the prior probability distribution $\\rho$ to have\n",
    "$$-\\ln \\rho(q) \\propto R(q) + \\ldots$$\n",
    "The ellipses denote (in the quantum mechanic's parlance) another infinite constant that we can set to zero.\n",
    "\n",
    "For the rest of this post, we'll be concerned exclusively with the case where $\\pi$ is a Gaussian measure.\n",
    "It's conventional to describe a Gaussian measure by its mean $m$ and its covariance operator $C$.\n",
    "In our case, however, we'll work instead with the *precision* operator $L$, which is equal to $C^{-1}$, the inverse of the covariance matrix.\n",
    "For problems posed in function spaces, the precision operator is often a linear differential operator with a concise expression.\n",
    "As we'll see shortly, the covariance matrix has to be a compact operator, which will mean that $L$ can be unbounded.\n",
    "In future posts we'll look at what happens when $\\pi$ is not Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proper priors\n",
    "\n",
    "In the deterministic inverse problems literature, it's common to use\n",
    "$$R(q) = \\frac{1}{2}\\int_\\Omega|\\nabla q|^2dx$$\n",
    "as a regularization functional.\n",
    "This accomplishes several goals.\n",
    "First, the inferred parameters obtained without regularization usually have spurious high-wavenumber noise.\n",
    "Adding this penalty filters out the noise.\n",
    "Second, it can guarantee a unique solution where the problem would be otherwise ill-posed.\n",
    "\n",
    "If we try to form a Gaussian measure $\\rho$ for which $-\\ln\\rho \\propto R(q) + \\ldots$, we'll run aground right away.\n",
    "Unless we also add boundary conditions to $q$, there is no constraint on the average value of $q$.\n",
    "We can add any constant we want to $q$ and the value of $R$ is unchanged.\n",
    "Viewing this in a probabilistic light now, the putative prior distribution doesn't integrate to 1 -- it is *improprer*.\n",
    "\n",
    "Strictly speaking, you can use an improper prior so long as the posterior is proper.\n",
    "The observational data need to constrain the constant mode and this does happen.\n",
    "So we would use an improper prior when we know nothing at all about what value the parameter can take and we expect that the data we have can give us that informaiton.\n",
    "\n",
    "Do we really have no prior information whatsoever about the average value of the parameters we want to infer?\n",
    "Let's take the example I mentioned above about inferring the density of material within the earth from satellite observations of earth's gravitational pull.\n",
    "Maybe you don't remember common rock densities off the top of your head, but you can go fetch one and weigh it and then put it in a beaker of water.\n",
    "I'll save you the trouble, it's probably got a density around 3000 kg/m${}^3$.\n",
    "Granted, the densities deep inside the earth are higher.\n",
    "Say you guessed that the plausible range spanned an order of magnitude in both directions: between 300 and 30,000 kg/m${}^3$.\n",
    "You wouldn't be doing particularly good but you also wouldn't be doing terrible either.\n",
    "So it beggars belief that we should use a prior for a geophysical inverse problem that is totally uninformative about the mean value.\n",
    "It might have high variance.\n",
    "It shouldn't be infinite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace class operators\n",
    "\n",
    "To prescribe a Gaussian measure, we need to know its mean $m$ and its precision operator $L$.\n",
    "The precision has to be symmetric and positive-definite.\n",
    "My objection above about properness of the prior is equivalent to the statement that $L$ has a zero eigenvalue.\n",
    "In the function space setting, there are more criteria.\n",
    "We don't think about them in finite dimensions because they're almost vacuously true.\n",
    "In the infinite-dimensional case they demand some thought (ugh).\n",
    "\n",
    "The first criterion is that the sum of all the eigenvalues of the covariance operator, i.e. the trace, is finite.\n",
    "In finite dimensions this is obvious.\n",
    "Being a trace class operator on a function space is special.\n",
    "For example, the identity operator or any unitary map is not trace class.\n",
    "Since we've chosen to work with the precision operator instead of the covariance, we can rephrase this condition as saying that the sum of the reciprocals of the eigenvalues of the precision has to be finite.\n",
    "\n",
    "#### Two choices of precision operator\n",
    "\n",
    "Suppose we wanted to do the bare minimum of effort to turn a determinstic inverse problem into a statistical one.\n",
    "Rather than use the improprer prior I showed above, we could instead use\n",
    "$$R(q) = \\frac{1}{2}\\int_\\Omega\\left(q^2 + \\gamma^2|\\nabla q|^2\\right)dx$$\n",
    "where $\\alpha$ is some length scale we have to choose.\n",
    "The precision operator is then\n",
    "$$L = I - \\gamma^2\\Delta,$$\n",
    "and we can write the more abstract form $R(q) = \\frac{1}{2}\\langle Lq, q\\rangle$.\n",
    "This choice of precision operator gets rid of the zero eigenvalue from before, so it gives us a proper prior.\n",
    "Is it true that $\\text{Tr}(L^{-1}) < \\infty$ for this choice of $L$?\n",
    "\n",
    "We can answer that question using Weyl's law.\n",
    "I want to be careful here about the units, so I'll adopt a slightly different convention and say that $\\lambda$, $\\phi$ are an eigenvalue / eigenfunction pair for $-\\Delta$ if\n",
    "$$-\\Delta\\phi = \\lambda^{-2}\\phi.$$\n",
    "This choice, instead of the more conventional one, makes $\\lambda$ have units of length.\n",
    "Weyl's law, as originally stated, gives an expression for the eigenvalue counting function:\n",
    "$$N(\\lambda) \\equiv \\#\\{\\text{eigenvalues of }-\\Delta\\text{ greater than }\\lambda\\} \\sim \\text{const}\\times\\text{vol}(\\Omega)\\times\\lambda^{-d}.$$\n",
    "If all you knew was that the asymptotics of the eigenvalue counting function have something to do with the volume of the domain (can you hear the size of the drum?) and that it has to go to infinity as $\\lambda$ goes to 0, you could guess at this purely from dimensional analysis.\n",
    "Figuring out what the constant prefactor and the remainder term are is harder but we won't need that much detail.\n",
    "\n",
    "Weyl's law then implies that the eigenvalues decay like\n",
    "$$\\lambda_n \\sim \\text{const}\\times\\text{vol}(\\Omega)^{1/d}\\times n^{-1/d}.$$\n",
    "So the eigenvalues of $(I - \\gamma^2\\Delta)^{-1}$ go like\n",
    "$$\\sigma_n = (1 + \\gamma^2\\lambda_n^{-2})^{-1} \\sim \\text{const}\\times\\frac{\\gamma^2}{\\text{vol}(\\Omega)^{2/d}} \\times n^{-2/d}.$$\n",
    "In dimension $d = 2$ the eigenvalues decay like $n^{-1}$.\n",
    "If we sum these, the trace diverges like the harmonic series.\n",
    "In 3D the divergence is even more rapid.\n",
    "\n",
    "Suppose we instead take the precision to look like the biharmonic operator plus lower-order terms:\n",
    "$$L = I - \\alpha^2\\Delta + \\gamma^4\\Delta^2$$\n",
    "where now we need to pick two length scales $\\alpha$, $\\gamma$.\n",
    "Weyl's law now comes to the rescue to show that the eigenvalues of $L^{-1}$ are instead asymptotic to\n",
    "$$\\sigma_n \\sim \\text{const} \\times \\frac{\\gamma^4}{\\text{vol}(\\Omega)^{4/d}} \\times n^{-4/d}$$\n",
    "which is summable in dimensions 2 and 3.\n",
    "\n",
    "#### Numerical demonstration\n",
    "\n",
    "Let's write some code to illustrate this.\n",
    "If we have a Gaussian random variable $\\xi$ with mean 0 and we multiply it by a matrix $\\Gamma$, the covariance of $\\Gamma\\xi$ is\n",
    "$$\\text{cov}(\\Gamma\\xi) = \\Gamma\\,\\text{cov}(\\xi)\\,\\Gamma^*.$$\n",
    "So if we want to make a random variable that has $L$ as its precision operator, we can do that by computing some kind of matrix square root of $L$.\n",
    "We can do that either with spectral theory or computing the Cholesky decomposition.\n",
    "The code below uses the Cholesky decomposition.\n",
    "But it's harder to scale Cholesky or other factorization approaches to smaller meshes for 3D problems and in that setting we would be more likely to use spectral theory approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import firedrake\n",
    "from firedrake import (\n",
    "    Constant, exp, inner, outer, avg, jump, grad, dx, ds, dS, assemble\n",
    ")\n",
    "from firedrake.petsc import PETSc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx, ny = 32, 32\n",
    "lx, ly = 1.0, 1.0\n",
    "Lx, Ly = Constant(lx), Constant(ly)\n",
    "mesh = firedrake.RectangleMesh(nx, ny, lx, ly, diagonal=\"crossed\")\n",
    "Q = firedrake.FunctionSpace(mesh, \"CG\", 2)\n",
    "area = assemble(Constant(1) * dx(mesh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is lifted directly from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        function_space,\n",
    "        covariance=None,\n",
    "        generator=np.random.default_rng()\n",
    "    ):\n",
    "        if covariance is None:\n",
    "            ϕ = firedrake.TrialFunction(function_space)\n",
    "            ψ = firedrake.TestFunction(function_space)\n",
    "            covariance = inner(ϕ, ψ) * dx\n",
    "\n",
    "        M = assemble(covariance, mat_type='aij').M.handle\n",
    "        ksp = PETSc.KSP().create()\n",
    "        ksp.setOperators(M)\n",
    "        ksp.setUp()\n",
    "\n",
    "        pc = ksp.pc\n",
    "        pc.setType(pc.Type.CHOLESKY)\n",
    "        pc.setFactorSolverType(PETSc.Mat.SolverType.PETSC)\n",
    "        pc.setFactorSetUpSolverType()\n",
    "        L = pc.getFactorMatrix()\n",
    "        pc.setUp()\n",
    "\n",
    "        self.rng = generator\n",
    "        self.function_space = function_space\n",
    "        self.preconditioner = pc\n",
    "        self.cholesky_factor = L\n",
    "\n",
    "        self.rhs = firedrake.Function(self.function_space)\n",
    "        self.noise = firedrake.Function(self.function_space)\n",
    "\n",
    "    def __call__(self):\n",
    "        z, ξ = self.rhs, self.noise\n",
    "        N = len(z.dat.data_ro[:])\n",
    "        z.dat.data[:] = self.rng.standard_normal(N)\n",
    "\n",
    "        L = self.cholesky_factor\n",
    "        with z.dat.vec_ro as Z:\n",
    "            with ξ.dat.vec as Ξ:\n",
    "                L.solveBackward(Z, Ξ)\n",
    "                Ξ *= np.sqrt(area / N)\n",
    "\n",
    "        return ξ.copy(deepcopy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll make an object to sample from the first random process, which uses the precision $I - \\gamma^2\\Delta$.\n",
    "The operator $L^{-1}$ is not trace-class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ϕ, ψ = firedrake.TestFunction(Q), firedrake.TrialFunction(Q)\n",
    "γ = firedrake.sqrt(Lx * Ly)\n",
    "M = (ϕ * ψ + γ**2 * inner(grad(ϕ), grad(ψ))) * dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1_generator = NoiseGenerator(\n",
    "    function_space=Q,\n",
    "    covariance=M,\n",
    "    generator=np.random.default_rng(1453),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'd like to use the biharmonic functional\n",
    "$$R(q) = \\int_\\Omega\\left(q^2 + \\gamma^4|\\nabla^2q|^2\\right)dx,$$\n",
    "which penalizes large values of the curvature $\\nabla^2q$.\n",
    "Conventional finite element basis functions are continuous and piecewise-differentiable, but their derivatives have jump discontinuities across cell boundaries.\n",
    "There are continuously-differentiable finite element bases which we could use to construct a conforming discretization of the curvature penalty.\n",
    "I'll instead use a non-conforming discretization based on ordinary CG elements.\n",
    "This approach is similar to how we used DG elements for the convection-diffusion equation.\n",
    "For that problem, we applied Nitsche's method at all of the cell boundaries in order to make the solution continuous.\n",
    "Here we'll instead apply Nitsche's method at all the cell boundaries to make the solution's gradient continuous.\n",
    "I'm partly following [this paper](https://doi.org/10.1515/jnma-2023-0028) for discretization of the curvature penalty but working back to a minimization form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ϕ = firedrake.Function(Q)\n",
    "Dϕ = grad(ϕ)\n",
    "DDϕ = grad(Dϕ)\n",
    "\n",
    "h = firedrake.FacetArea(mesh)\n",
    "vol = firedrake.CellVolume(mesh)\n",
    "\n",
    "α = firedrake.Constant(4.0)\n",
    "k = firedrake.Constant(Q.ufl_element().degree())\n",
    "β = 3 * α * k * (k - 1) / 8 * avg(h)**2 * avg(1 / vol)\n",
    "β_Γ = 3 * α * k * (k - 1) * h**2 / vol\n",
    "\n",
    "ν = firedrake.FacetNormal(mesh)\n",
    "J_cells = inner(DDϕ, DDϕ) * dx\n",
    "J_facets = avg(inner(DDϕ, outer(ν, ν))) * jump(Dϕ, ν) * dS\n",
    "J_facet_penalty = β / avg(h) * jump(Dϕ, ν)**2 * dS\n",
    "J_boundary = inner(Dϕ, ν) * inner(DDϕ, outer(ν, ν)) * ds\n",
    "J_boundary_penalty = β_Γ / h * inner(Dϕ, ν)**2 * ds\n",
    "\n",
    "J_Δ = 0.5 * (J_cells - J_facets - J_boundary + J_facet_penalty + J_boundary_penalty)\n",
    "J_2 = 0.5 * ϕ**2 * dx\n",
    "\n",
    "J = J_2 + γ**4 * J_Δ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firedrake import derivative\n",
    "\n",
    "M = derivative(derivative(J, ϕ), ϕ)\n",
    "h2_generator = NoiseGenerator(\n",
    "    function_space=Q,\n",
    "    covariance=M,\n",
    "    generator=np.random.default_rng(seed=1666),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots below show samples from the precision operator $I - \\gamma^2\\Delta$ first and $I + \\gamma^4\\Delta^2$ second.\n",
    "Maybe the most compelling argument I can give you for why biharmonic regularization is the way to go is, just look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=3, subplot_kw={\"projection\": \"3d\"})\n",
    "for ax in axes.flatten():\n",
    "    w = h1_generator()\n",
    "    firedrake.trisurf(w, axes=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=3, subplot_kw={\"projection\": \"3d\"})\n",
    "for ax in axes.flatten():\n",
    "    z = h2_generator()\n",
    "    firedrake.trisurf(z, axes=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the biharmonic operator is much less common than the Laplacian.\n",
    "I have a hunch that this is because it's more difficult to implement and not because it doesn't give better results.\n",
    "You can pick good values of the penalty parameters based on the mesh quality and polynomial degree -- you don't need to, for example, iterate on them based on candidate solutions.\n",
    "But this knowledge isn't disseminated very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposal mechanism\n",
    "\n",
    "Questions about what kinds of covariance operator to choose concern the problem that we wish to solve.\n",
    "The final challenge I ran into is not about what problem to solve but how to solve it.\n",
    "We've only talked so far about what prior to use and not about Markov chain Monte Carlo methods as such.\n",
    "By way of recap, what an MCMC algorithm needs is a *proposal generation kernel* describing how to generate a new candidate state $q_2$ from the current state $q_1$:\n",
    "$$Q(q_1\\rightarrow q_2) = \\text{probability of proposing }q_2\\text{ given the current state }q_1.$$\n",
    "At each step, we generate a new candidate and either accept or reject it according to some rule\n",
    "$$\\alpha(q_1\\rightarrow q_2) = \\text{probability of accepting }q_2\\text{ given the current state }q_1.$$\n",
    "The most common rule is Metropolis-Hastings.\n",
    "There are other possible rules, like the [Barker rule](https://doi.org/10.1111/rssb.12482).\n",
    "The combination of a good proposal mechanism and an accept-reject step gives a stochastic process with the desired distribution as its unique stable steady state.\n",
    "\n",
    "All of the advanced MCMC sampling algorithms work by using more and more sophisticated proposal generation kernels.\n",
    "A common thread is to use a Langevin-type equation to generate samples:\n",
    "$$\\dot q = -\\frac{1}{2}K^{-1}\\nabla\\log\\pi + \\dot w$$\n",
    "where $K$ is some s.p.d. linear operator.\n",
    "The noise forcing has\n",
    "$$\\text{cov}(w(s), w(t)) = \\delta(s - t)K^{-1}.$$\n",
    "Discretize the ODE however you like and you get a proposal mechanism.\n",
    "Seems fine but isn't.\n",
    "In infinite dimensions, we can only use one discretization scheme.\n",
    "To understand why, we need to think (ugh) about measure theory (yech)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Absolute continuity of proposal mechanism\n",
    "\n",
    "If we want to generate samples from a distribution $\\pi$ and we've chosen a proposal kernel $Q$, the Metropolis-Hastings accept-reject formula is\n",
    "$$\\alpha(q_1 \\rightarrow q_2) = \\min\\left\\{1,\\frac{\\pi(q_2)Q(q_2\\rightarrow q_1)}{\\pi(q_1)Q(q_1\\rightarrow q_2)}\\right\\}.$$\n",
    "(As an aside, I never really understood this on a gut level until I read [this paper](https://www.jstor.org/stable/3182774).)\n",
    "There are technical requirements on the proposal mechanism that we get to ignore in finite dimensions but have to pay attention to in function spaces.\n",
    "For example, in finite dimensions we can often take $Q$ to be a normal random variable with mean $q_1$.\n",
    "This proposal might converge so slow as to be useless for practical purposes, but it won't break the theory.\n",
    "In infinite dimensions, reasonable-looking proposals break the theory.\n",
    "\n",
    "The technical requirements on the proposal mechanism are laid out most clearly in [Tierney (1998)](https://www.jstor.org/stable/2667233).\n",
    "First, note that the proposal generation kernel is a mapping from the current state $q_1$ to a probability density w.r.t. $q_2$.\n",
    "It isn't a probability density as such, only a mapping whose outputs are probability densities.\n",
    "Now we'll define a probability density over pairs $q_1$, $q_2$:\n",
    "$$R(q_1\\rightarrow q_2) = \\pi(q_1)Q(q_1\\rightarrow q_2).$$\n",
    "This density describes the probability, assuming that we had converged to the true equilibrium density, of starting at $q_1$ and proposing $q_2$.\n",
    "Again, this is a joint density over both $q_1$ and $q_2$, not a mere mapping from $q_1$ to a density over $q_2$.\n",
    "Mind you, we don't have access to $\\pi$ as such -- that was the whole point of the exercise.\n",
    "\n",
    "Now we can ask what the relationship is between the forward and the *reverse* density\n",
    "$$R(q_1 \\leftarrow q_2) = R(q_2 \\rightarrow q_1).$$\n",
    "This describes the probability of starting out instead at $q_2$ and proposing $q_1$.\n",
    "Remember that a Markov chain in statistical equilibrium settles into a state of detailed balance -- the probability of starting at one state and ending up at another is the same as going backwards.\n",
    "\n",
    "Here's the technical condition which is so hard to achieve in practice and which Tierney proves.\n",
    "The proposal mechanism $Q$ does not need to be exactly in detailed balance -- after all, that's what the accept-reject mechanism is supposed to do for us.\n",
    "**But the forward proposal density cannot, with positive probability, suggest a new state where the reverse move has zero probability under the reverse proposal density.**\n",
    "\n",
    "There's fancy measure theory jargon for the condition I'm talking about: the proposal density needs to be [absolutely continuous](https://en.wikipedia.org/wiki/Absolute_continuity) with respect to the reverse proposal.\n",
    "In the finite-dimensional case, it's hard to even imagine how you could fuck up so badly that your proposal mechanism has zero probability of inhabiting part of the state space.\n",
    "A pure random walk -- taking the proposal to be a standard normal random variable -- won't fall into this trap.\n",
    "It might converge real slow, but it can at least get from anywhere to anywhere.\n",
    "\n",
    "We saw earlier that making sure the covariance is a trace-class operator is not a guaranteed proposition.\n",
    "You might guess that finding a forward proposal generation kernel that isn't singular w.r.t. its reverse is also not guaranteed.\n",
    "You'd be right.\n",
    "In the function space setting, *almost no proposal mechanisms are absolutely continuous*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Absolute continuity of Gaussian measures\n",
    "\n",
    "We need to unpack the [Feldman-Hájek theorem](https://en.wikipedia.org/wiki/Feldman%E2%80%93H%C3%A1jek_theorem) in order to understand this.\n",
    "It's kind of a doozy and I don't find the form that it's usually stated in to be easy to grasp at first sight.\n",
    "\n",
    "A Gaussian measure is defined entirely by its mean and precision operator.\n",
    "We'll assume that the means are zero in order to simplify things and because we won't need to consider non-zero means.\n",
    "Suppose we have two Gaussian measures on $H$ with precision operators $L$ and $K$.\n",
    "The precision operators have eigendecompositions; the eigenfunctions won't matter because we can map one basis into another with a unitary operator.\n",
    "We'll let $\\sigma_n(L)$, $\\sigma_n(K)$ be the $n$-th eigenvalue of each operator.\n",
    "The Feldman-Hájek theorem tells us that the two measures are mutually equivalent if and only if\n",
    "$$\\sum_n\\left(\\frac{\\sigma_n(K)}{\\sigma_n(L)} - 1\\right)^2 < \\infty.$$\n",
    "Otherwise, the two measures are mutually singular.\n",
    "In short, the two eigenvalue sequences need to grow at the same rate even including the same constant multiplicative factor.\n",
    "For example, suppose that $K = a\\cdot L$ for some scalar $a$ that's not equal to 1.\n",
    "Then $\\sigma_n(K)/\\sigma_n(L) - 1 = a - 1$.\n",
    "For random variables taking values with values in a function space, the two distributions are mutually singular.\n",
    "This is not like the finite-dimensional case at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MCMC in function spaces\n",
    "\n",
    "Now let's discretize the Langevin equation.\n",
    "We'll assume again that $-\\log\\pi = \\frac{1}{2}\\langle Lq, q\\rangle$.\n",
    "The simplest family of schemes we might come up with are the $\\theta$ schemes:\n",
    "$$\\frac{q_{n + 1} - q_n}{\\delta t} = -\\frac{1}{2}K^{-1}L\\left((1 - \\theta)q_n + \\theta\\,q_{n + 1}\\right) + \\delta t^{-1/2}\\,\\delta w_n.$$\n",
    "Rearranging terms, we get\n",
    "$$q_{n + 1} = \\left(I + \\frac{\\theta\\,\\delta t}{2}K^{-1}L\\right)^{-1}\\left\\{\\left(I - \\frac{(1 - \\theta)\\delta t}{2}K^{-1}L\\right)q_n + \\delta t^{-1/2}\\,\\delta w_n\\right\\}.$$\n",
    "That's kind of a lot.\n",
    "We can learn something useful by assuming that $K = L$, i.e. that we can exactly invert the precision operator.\n",
    "In this case, we get\n",
    "$$q_{n + 1} = \\frac{1 - (1 - \\theta)\\delta t/2}{1 + \\theta\\,\\delta t/2}q_n + \\frac{\\sqrt{\\delta t}}{1 + \\theta\\,\\delta t/2}\\delta w_n.$$\n",
    "(Remember that now $\\text{cov}(\\delta w_n) = L^{-1}$.)\n",
    "The proposal is a linear combination of Gaussian random variables, so it's also Gaussian and we can ask what its covariance is:\n",
    "$$\\text{cov}(q_{n + 1}) = \\frac{(1 - (1 - \\theta)\\delta t / 2))^2}{(1 + \\theta\\,\\delta t/2)^2}\\text{cov}(q_n) + \\frac{\\delta t}{(1 + \\theta\\,\\delta t/2)^2}\\text{cov}(\\delta w_n) = \\ldots$$\n",
    "We know from the outset that $\\text{cov}(\\delta W_n) = L^{-1}$.\n",
    "Now suppose that the chain had converged, in which case $\\text{cov}(q_n) = L^{-1}$ as well.\n",
    "A bit of routine algebra gives\n",
    "$$\\ldots = \\frac{(1 + \\theta\\,\\delta t / 2)^2 + (1 - 2\\theta)\\,\\delta t^2 / 4}{(1 + \\theta\\,\\delta t/2)^2}L^{-1}.$$\n",
    "When $\\theta = 1/2$, this all reduces to $L^{-1}$.\n",
    "For any other value of $\\theta$, the covariance of the proposal is some non-trivial scalar multiple of $L^{-1}$.\n",
    "The Feldman-Hájek theorem then tells us that the forward proposal density $R(q_1\\rightarrow q_2)$ is singular w.r.t. its reversal $R(q_1 \\leftarrow q_2)$.\n",
    "The theory of MCMC on general state spaces then tells us that the resulting Markov chain would not converge.\n",
    "\n",
    "In short, **only $\\theta = 1/2$ can give a convergent Markov chain.**\n",
    "If we do take $\\theta = 1/2$, we can define\n",
    "$$\\alpha = \\frac{1 - \\delta t/4}{1 + \\delta t / 4},$$\n",
    "in which case we can concisely express the proposal generation mechanism as\n",
    "$$q_{n + 1} = \\alpha q_n + (1 - \\alpha^2)^{1/2}\\,\\delta w_n$$\n",
    "where again the $\\delta w_n$ are uncorrelated Gaussian random variables with $\\text{cov}(\\delta w_n) = L^{-1}$.\n",
    "We used an unholy mess of sophisticated math and bandied about some scary words like \"Langevin\".\n",
    "But in the end all we did was reinvent an [AR(1) process](https://en.wikipedia.org/wiki/Autoregressive_model).\n",
    "Embarrassing.\n",
    "\n",
    "We made the simplifying assumption earlier that we could take our preconditioning operator $K$ to be equal to the exact precision $L$.\n",
    "What if we don't want to assume that anymore?\n",
    "To make some of the notation easier, introduce the matrix\n",
    "$$S = I + \\frac{\\theta\\,\\delta t}{2}K^{-1}L.$$\n",
    "If you work through some heinous matrix algebra, you can show that\n",
    "$$\\text{cov}(q_{n + 1}) = L^{-1} + \\frac{(1 - 2\\theta)\\,\\delta t^2}{4}S^{-1}K^{-1}LK^{-*}S^{-*}.$$\n",
    "All of this simplifies to $L^{-1}$ if $\\theta = 1/2$.\n",
    "Otherwise, it's not equal to $L^{-1}$ and we run the risk of getting a singular proposal generation mechanism again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "\n",
    "Before, we assumed that we could compute a matrix square root of $L$.\n",
    "In 2D we can do this using the Cholesky factorization, but in 3D we would need iterative methods.\n",
    "Suppose we considered it impossible or too aggravating to write a procedure to apply $L^{-1/2}$.\n",
    "For example, say we insisted on using $L = I + \\gamma^4\\Delta^2$.\n",
    "We could get even more exotic and include boundary terms:\n",
    "$$\\langle Lq, q\\rangle = \\int_\\Omega\\left(q^2 + \\gamma^4|\\nabla^2q|^2\\right)dx + \\kappa\\int_{\\partial\\Omega}q^2ds.$$\n",
    "This isn't equal to a square of operators like $(I - \\gamma^2\\Delta)^2$ -- that would be too convenient.\n",
    "We could take\n",
    "$$G = I - \\gamma^2\\Delta$$\n",
    "and then use\n",
    "$$K = G^2.$$\n",
    "Note that $G$ is self-adjoint.\n",
    "This operator isn't equal to $L$ but it does have the same spectral asymptotics.\n",
    "The lower-order differential operators and the boundary terms don't matter.\n",
    "We then generate proposal through\n",
    "$$\\begin{align}\n",
    "z & = \\left(I - \\frac{\\delta t}{4}G^{-2}L\\right)q_n + \\delta t^{-1/2}G^{-1}\\xi_n \\\\\n",
    "q_{n + 1} & = \\left(I + \\frac{\\delta t}{4}G^{-2}L\\right)^{-1}z\n",
    "\\end{align}$$\n",
    "where the vector $\\xi_n$ are i.i.d. standard normals.\n",
    "There are lots of applications of $G^{-1}$ here.\n",
    "As a first pass, we can do all these sequentially.\n",
    "If you wanted to optimize this, you can solve a linear equation with the same matrix but multiple right-hand sides faster than doing repeated solves with a single right-hand side.\n",
    "Since $G$ and $L$ are all symmetric and positive-definite and $G^2$ and $L$ are spectrally equivalent, we expect that the operator $I + \\delta t\\,G^{-2}L/4$ is going to be relatively easy to invert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firedrake",
   "language": "python",
   "name": "firedrake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "nikola": {
   "category": "",
   "date": "2025-04-06 13:17:30 UTC-07:00",
   "description": "",
   "link": "",
   "slug": "biharmonic-priors",
   "tags": "",
   "title": "Biharmonic priors",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
