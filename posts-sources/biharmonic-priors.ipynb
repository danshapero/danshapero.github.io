{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous post, I described a procedure for sampling from probability distributions using Monte Carlo methods.\n",
    "One of the subtle points that I tried to address there is what happens when the quantity we want to sample is a function of space and time.\n",
    "For example, we might want to infer the conductivity of an aquifer from well head measurements; the material density in the earth's subsurface from satellite gravimetry; or basal drag between a glacier and the landscape from measurements of its surface velocity.\n",
    "The parameter space is now a space of functions, and so it has infinitely many dimensions.\n",
    "Extending common algorithms to work in function spaces is hard.\n",
    "\n",
    "Since writing that first post, I read [this paper](https://doi.org/10.1214/13-STS421) by Cotter and others.\n",
    "I think my previous post was wrong now.\n",
    "Here I'd like to explain why (it involves Weyl's law) and how to fix it (it involves Nitsche's method).\n",
    "\n",
    "The failure is trying to bolt a statistical interpretation onto deterministic inverse problems.\n",
    "I'm guilty of this.\n",
    "I'd argue that many textbooks on inverse problems are too.\n",
    "One misstep is to assume that common choices of regularization functional used in the deterministic inverse problems literature are going to translate into sensible prior distributions.\n",
    "They don't.\n",
    "\n",
    "To clarify the notation for later, we're interested in inferring a field $q$ defined over some spatial domain $\\Omega$.\n",
    "In a deterministic inverse problem, we would add a multiple of a functional $R(q)$ to the objective in order to regularize the problem.\n",
    "The conventional approach is to use a multiple of\n",
    "$$R(q) = \\frac{1}{2}\\int_\\Omega|\\nabla q|^2dx.$$\n",
    "We can make this look like a statistical inference problem by taking the prior probability distribution $\\rho$ to have\n",
    "$$-\\ln \\rho(q) \\propto R(q) + \\ldots$$\n",
    "The ellipses denote (in the quantum mechanic's parlance) another infinite constant that we can set to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory\n",
    "\n",
    "Our goal is to construct a procedure for sampling from a probability distribution $\\pi(q)$.\n",
    "We assume that the parameters $q$ live in a separable Hilbert space $H$.\n",
    "When $H$ is infinite-dimensional, there are a couple of hard parts that don't occur in finite dimensions.\n",
    "\n",
    "For the rest of this post, we'll be concerned exclusively with the case where $\\pi$ is a Gaussian measure.\n",
    "In future posts we'll look at what happens when it isn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proper priors\n",
    "\n",
    "In the deterministic inverse problems literature, it's common to use\n",
    "$$R(q) = \\frac{1}{2}\\int_\\Omega|\\nabla q|^2dx$$\n",
    "as a regularization functional.\n",
    "This accomplishes several goals.\n",
    "First, the inferred parameters obtained without regularization usually have spurious high-wavenumber noise.\n",
    "Adding this penalty filters out the noise.\n",
    "Second, it can guarantee a unique solution where the problem would be otherwise ill-posed.\n",
    "\n",
    "If we try to form a Gaussian measure $\\rho$ for which $-\\ln\\rho \\propto R(q) + \\ldots$, we'll run aground right away.\n",
    "Unless we also add boundary conditions to $q$, there is no constraint on the average value of $q$.\n",
    "We can add any constant we want to $q$ and the value of $R$ is unchanged.\n",
    "Viewing this in a probabilistic light now, the putative prior distribution doesn't integrate to 1 -- it is *improprer*.\n",
    "\n",
    "Strictly speaking, you can use an improper prior so long as the posterior is proper.\n",
    "The observational data need to constrain the constant mode and this does happen.\n",
    "So we would use an improper prior when we know nothing at all about what value the parameter can take and we expect that the data we have can give us that informaiton.\n",
    "\n",
    "Do we really have no prior information whatsoever about the average value of the parameters we want to infer?\n",
    "Let's take the example I mentioned above about inferring the density of material within the earth from satellite observations of earth's gravitational pull.\n",
    "Maybe you don't remember common rock densities off the top of your head, but you can go fetch one and weigh it and then put it in a beaker of water.\n",
    "I'll save you the trouble, it's probably got a density around 3000 kg/m${}^3$.\n",
    "Granted, the densities deep inside the earth are higher.\n",
    "Say you guessed that the range went from an order of magnitude in both directions: between 300 and 30,000 kg/m${}^3$.\n",
    "You wouldn't be doing particularly good but you also wouldn't be doing terrible either.\n",
    "So it beggars belief that we should use a prior for a geophysical inverse problem that is totally uninformative about the mean value.\n",
    "It might have high variance.\n",
    "It shouldn't be infinite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trace class operators\n",
    "\n",
    "To prescribe a Gaussian measure, we need to know its mean and covariance.\n",
    "The covariance has to be symmetric and positive-definite.\n",
    "(You can phrase my objection about properness above into questions about the nature of the covariance.)\n",
    "But in the function space setting, there are other criteria.\n",
    "We don't think about them in finite dimensions because they're almost vacuously true.\n",
    "In the infinite-dimensional case they demand some thought (ugh).\n",
    "\n",
    "The first criterion is that the sum of all the eigenvalues of the covariance operator, i.e. the trace, is finite.\n",
    "In finite dimensions this is obvious.\n",
    "In function spaces, being a trace class operator is special.\n",
    "For example, the identity operator or any unitary map is not trace class.\n",
    "\n",
    "Suppose we wanted to make the most minimal adaptation of a determinstic inverse problem into a statistical one.\n",
    "Rather than use the improprer prior I showed above, we instead use\n",
    "$$R(q) = \\frac{1}{2}\\int_\\Omega\\left(q^2 + \\alpha^2|\\nabla q|^2\\right)dx$$\n",
    "where $\\alpha$ is some length scale we have to choose.\n",
    "If we define the operator\n",
    "$$L = I - \\alpha^2\\Delta,$$\n",
    "then $R(q) = \\frac{1}{2}\\langle Lq, q\\rangle$.\n",
    "The covariance is then equal to $L^{-1}$; the jargon is that $L$ is the *precision* operator.\n",
    "Is $L^{-1}$ of trace class?\n",
    "\n",
    "We can answer that question using Weyl's law.\n",
    "I want to be careful here about the units, so I'll adopt a slightly different convention and say that $\\lambda$, $\\phi$ are an eigenvalue / eigenfunction pair for $-\\Delta$ if\n",
    "$$-\\Delta\\phi = \\lambda^{-2}\\phi.$$\n",
    "This choice, instead of the more conventional one, makes $\\lambda$ have units of length.\n",
    "Weyl's law then implies that the eigenvalues decay like\n",
    "$$\\lambda_n \\sim \\text{const}\\times\\text{vol}(\\Omega)^{1/d}\\times n^{-1/d}.$$\n",
    "I'm being so fussy about the units because you can guess at this formula from dimensional analysis.\n",
    "So the eigenvalues of $(I - \\alpha^2\\Delta)^{-1}$ go like\n",
    "$$\\sigma_n = (1 + \\alpha^2\\lambda_n^{-2})^{-1} \\sim \\text{const}\\times\\frac{\\alpha^2}{\\text{vol}(\\Omega)^{2/d}} \\times n^{-2/d}.$$\n",
    "In dimension $d = 2$ the eigenvalues decay like $n^{-1}$.\n",
    "The trace diverges like the harmonic series.\n",
    "In 3D the divergence is even more rapid.\n",
    "\n",
    "Suppose we instead take the precision to have the biharmonic operator as the leading term:\n",
    "$$L = I - \\alpha_1^2\\Delta + \\alpha_2^2\\Delta^2$$\n",
    "where now we need to pick two length scales $\\alpha_1$, $\\alpha_2$.\n",
    "Then Weyl's law now comes to the rescue to show that the eigenvalues of $L^{-1}$ are instead asymptotic to\n",
    "$$\\sigma_n \\sim \\text{const} \\times n^{-4/d}$$\n",
    "which is summable in dimensions 2 and 3.\n",
    "\n",
    "Further down, I'll show some samples generated from both the wrong $I - \\alpha^2\\Delta$ prior and the biharmonic prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proposal mechanism\n",
    "\n",
    "Questions about what kinds of covariance operator to choose concern the problem that we wish to solve.\n",
    "The final challenge I ran into is not about what problem to solve but how to solve it.\n",
    "A common thread in the advanced MCMC sampling literature is to use a Langevin-type equation to generate samples:\n",
    "$$\\dot q = -\\frac{1}{2}K^{-1}\\nabla\\log\\pi + K^{-1/2}\\dot W$$\n",
    "where $G$ is some s.p.d. linear operator.\n",
    "Discretize the ODE however you like and you get a proposal mechanism.\n",
    "\n",
    "Seems fine but isn't.\n",
    "In infinite dimensions, we can only use one discretization scheme.\n",
    "To understand why, we need to think (ugh) about measure theory (yech).\n",
    "\n",
    "Our goal was to sample from some intractable probability distribution $\\pi$.\n",
    "MCMC algorithms construct computable Markov chains whose limiting distribution is $\\pi$.\n",
    "The Metropolis-Hastings algorithm works by combining a proposal mechanism -- a random process that suggests new states of the Markov chain from the current value -- with an accept-reject step.\n",
    "This Markov chain is in statistical equilibrium when the probability of going from $q_1$ to $q_2$ is the same as the probability of going backward.\n",
    "In other words, if $P(q_1 \\rightarrow q_2)$ is the transition kernel for Metropolis-Hastings, then\n",
    "$$\\pi(q_1)P(q_1\\rightarrow q_2) = \\pi(q_2)P(q_2\\rightarrow q_1).$$\n",
    "At each step, we use a proposal density $Q(q_1\\rightarrow q_2)$ to select our next guess.\n",
    "We then either accept or reject the proposal according to whether it is more or less likely under the posterior distribution, weighted by the dynamics of the proposal mechanism:\n",
    "$$\\alpha(q \\rightarrow q^*) = \\min\\left\\{1,\\frac{\\pi(q^*)Q(q^*\\rightarrow q)}{\\pi(q)Q(q\\rightarrow q^*)}\\right\\}.$$\n",
    "(As an aside, I never really understood this on a gut level until I read [this paper](https://www.jstor.org/stable/3182774).)\n",
    "\n",
    "Let's get pedantic.\n",
    "What if the numerator or denominator is zero?\n",
    "Some of the state space would be inaccessible.\n",
    "The Markov chain wouldn't converge to the limiting density we want.\n",
    "There's fancy measure theory jargon for the condition I'm talking about: the proposal density needs to be [absolutely continuous](https://en.wikipedia.org/wiki/Absolute_continuity) with respect to the reverse proposal.\n",
    "In the finite-dimensional case, it's hard to even imagine how you could fuck up so badly that your proposal mechanism has zero probability of inhabiting part of the state space.\n",
    "A pure random walk -- taking the proposal to be a standard normal random variable -- won't fall into this trap.\n",
    "It might converge real slow, but it can at least get from anywhere to anywhere.\n",
    "But in the function space setting, *almost no proposal mechanisms are absolutely continuous*.\n",
    "\n",
    "To understand this, we need to unpack the [Feldman-Hájek theorem](https://en.wikipedia.org/wiki/Feldman%E2%80%93H%C3%A1jek_theorem), which is kind of a doozy.\n",
    "\n",
    "Now let's discretize the Langevin equation.\n",
    "We'll assume again that $-\\log\\pi = \\frac{1}{2}\\langle Lq, q\\rangle$.\n",
    "The simplest family of schemes we might come up with are the $\\theta$ schemes:\n",
    "$$\\frac{q_{n + 1} - q_n}{\\delta t} = -\\frac{1}{2}K^{-1}L\\left((1 - \\theta)q_n + \\theta\\,q_{n + 1}\\right) + \\delta t^{-1/2}\\,K^{-1/2}\\delta W_n.$$\n",
    "Rearranging terms, we get\n",
    "$$q_{n + 1} = \\left(I + \\frac{\\theta\\,\\delta t}{2}K^{-1}L\\right)^{-1}\\left\\{\\left(I - \\frac{(1 - \\theta)\\delta t}{2}K^{-1}L\\right)q_n + \\delta t^{-1/2}\\,K^{-1/2}\\delta W_n\\right\\}.$$\n",
    "That's kind of a lot.\n",
    "We can learn something useful by assuming that $K = L$, i.e. that we can exactly invert the precision operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "\n",
    "Now let's write some code to try and put this into practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import firedrake\n",
    "from firedrake import (\n",
    "    Constant, exp, inner, outer, avg, jump, grad, dx, ds, dS, assemble\n",
    ")\n",
    "from firedrake.petsc import PETSc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx, ny = 32, 32\n",
    "lx, ly = 1.0, 1.0\n",
    "Lx, Ly = Constant(lx), Constant(ly)\n",
    "mesh = firedrake.RectangleMesh(nx, ny, lx, ly, diagonal=\"crossed\")\n",
    "Q = firedrake.FunctionSpace(mesh, \"CG\", 2)\n",
    "area = assemble(Constant(1) * dx(mesh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is copied directly from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseGenerator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        function_space,\n",
    "        covariance=None,\n",
    "        generator=np.random.default_rng()\n",
    "    ):\n",
    "        if covariance is None:\n",
    "            ϕ = firedrake.TrialFunction(function_space)\n",
    "            ψ = firedrake.TestFunction(function_space)\n",
    "            covariance = inner(ϕ, ψ) * dx\n",
    "\n",
    "        M = assemble(covariance, mat_type='aij').M.handle\n",
    "        ksp = PETSc.KSP().create()\n",
    "        ksp.setOperators(M)\n",
    "        ksp.setUp()\n",
    "\n",
    "        pc = ksp.pc\n",
    "        pc.setType(pc.Type.CHOLESKY)\n",
    "        pc.setFactorSolverType(PETSc.Mat.SolverType.PETSC)\n",
    "        pc.setFactorSetUpSolverType()\n",
    "        L = pc.getFactorMatrix()\n",
    "        pc.setUp()\n",
    "\n",
    "        self.rng = generator\n",
    "        self.function_space = function_space\n",
    "        self.preconditioner = pc\n",
    "        self.cholesky_factor = L\n",
    "\n",
    "        self.rhs = firedrake.Function(self.function_space)\n",
    "        self.noise = firedrake.Function(self.function_space)\n",
    "\n",
    "    def __call__(self):\n",
    "        z, ξ = self.rhs, self.noise\n",
    "        N = len(z.dat.data_ro[:])\n",
    "        z.dat.data[:] = self.rng.standard_normal(N)\n",
    "\n",
    "        L = self.cholesky_factor\n",
    "        with z.dat.vec_ro as Z:\n",
    "            with ξ.dat.vec as Ξ:\n",
    "                L.solveBackward(Z, Ξ)\n",
    "                Ξ *= np.sqrt(area / N)\n",
    "\n",
    "        return ξ.copy(deepcopy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll make an object to sample from the first random process, which uses the precision $I - \\alpha^2\\Delta$.\n",
    "The operator $L^{-1}$ is not trace-class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ϕ, ψ = firedrake.TestFunction(Q), firedrake.TrialFunction(Q)\n",
    "ℓ = firedrake.sqrt(Lx * Ly)\n",
    "M = (ϕ * ψ + ℓ**2 * inner(grad(ϕ), grad(ψ))) * dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1_generator = NoiseGenerator(\n",
    "    function_space=Q,\n",
    "    covariance=M,\n",
    "    generator=np.random.default_rng(1453),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'd like to use the functional\n",
    "$$R(q) = \\int_\\Omega\\left(q^2 + \\lambda^2|\\nabla^2q|^2\\right)dx$$\n",
    "which penalizes large values of the curvature $\\nabla^2q$.\n",
    "Conventional finite element basis functions are continuous and piecewise-differentiable, but their derivatives have jump discontinuities across cell boundaries.\n",
    "There are continuously-differentiable finite element bases which we could use to construct a conforming discretization of the curvature penalty.\n",
    "I'll instead use a non-conforming discretization based on ordinary CG elements.\n",
    "This approach is similar to how we used DG elements for the convection-diffusion equation.\n",
    "For that problem, we applied Nitsche's method at all of the cell boundaries in order to make the solution continuous.\n",
    "Here we'll instead apply Nitsche's method at all the cell boundaries to make the solution's gradient continuous.\n",
    "I'm partly following [this paper](https://doi.org/10.1515/jnma-2023-0028) for discretization of the curvature penalty but working back to a minimization form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ϕ = firedrake.Function(Q)\n",
    "\n",
    "λ = firedrake.Constant(1.0)\n",
    "Dϕ = grad(ϕ)\n",
    "DDϕ = grad(Dϕ)\n",
    "\n",
    "h = firedrake.FacetArea(mesh)\n",
    "vol = firedrake.CellVolume(mesh)\n",
    "\n",
    "α = firedrake.Constant(4.0)\n",
    "k = firedrake.Constant(Q.ufl_element().degree())\n",
    "β = 3 * α * k * (k - 1) / 8 * avg(h)**2 * avg(1 / vol)\n",
    "β_Γ = 3 * α * k * (k - 1) * h**2 / vol\n",
    "\n",
    "ν = firedrake.FacetNormal(mesh)\n",
    "J_cells = inner(DDϕ, DDϕ) * dx\n",
    "J_facets = avg(inner(DDϕ, outer(ν, ν))) * jump(Dϕ, ν) * dS\n",
    "J_facet_penalty = β / avg(h) * jump(Dϕ, ν)**2 * dS\n",
    "J_boundary = inner(Dϕ, ν) * inner(DDϕ, outer(ν, ν)) * ds\n",
    "J_boundary_penalty = β_Γ / h * inner(Dϕ, ν)**2 * ds\n",
    "\n",
    "J_Δ = 0.5 * (J_cells - J_facets - J_boundary + J_facet_penalty + J_boundary_penalty)\n",
    "J_2 = 0.5 * ϕ**2 * dx\n",
    "\n",
    "J = J_2 + λ ** 4 * J_Δ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firedrake import derivative\n",
    "\n",
    "M = derivative(derivative(J, ϕ), ϕ)\n",
    "h2_generator = NoiseGenerator(\n",
    "    function_space=Q,\n",
    "    covariance=M,\n",
    "    generator=np.random.default_rng(seed=1666),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots below show samples from the precision operator $I - \\alpha^2\\Delta$ first and $I + \\alpha^4\\Delta^2$ second.\n",
    "Maybe the most compelling argument I can give you for why biharmonic regularization is the way to go is, just look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=3, subplot_kw={\"projection\": \"3d\"})\n",
    "for ax in axes.flatten():\n",
    "    w = h1_generator()\n",
    "    firedrake.trisurf(w, axes=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=3, subplot_kw={\"projection\": \"3d\"})\n",
    "for ax in axes.flatten():\n",
    "    z = h2_generator()\n",
    "    firedrake.trisurf(z, axes=ax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firedrake",
   "language": "python",
   "name": "firedrake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "nikola": {
   "category": "",
   "date": "2025-04-06 13:17:30 UTC-07:00",
   "description": "",
   "link": "",
   "slug": "biharmonic-priors",
   "tags": "",
   "title": "Biharmonic priors",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
